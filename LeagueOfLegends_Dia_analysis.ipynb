{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# League of Legends: Diamond Ranked Games (10 min) Analysis and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "League of Legends is a multiplayer online battle arena video game for Windows computer systems. In each game, two teams of five players face off in five different defined roles. The map consists of three main lanes along with a jungle area and the goal of the match is the destroy the enemy base ('Nexus'). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data:\n",
    "\n",
    "From dataset explanation: \"This dataset contains the first 10min. stats of approx. 10k ranked games (SOLO QUEUE) from a high ELO (DIAMOND I to MASTER). Players have roughly the same level\". A few notes here: 1) These games consist of data spread across 8 different ranks (Diamond 1 to Master) 2) Players being around the same level implies that these players are have about equaivalent experirence on current account. \n",
    "\n",
    "#### Data Terms:\n",
    "    Warding totem: An item that a player can put on the map to reveal the nearby area. Very useful for map/objectives control.\n",
    "    Minions: NPC that belong to both teams. They give gold when killed by players.\n",
    "    Jungle minions: NPC that belong to NO TEAM. They give gold and buffs when killed by players.\n",
    "    Elite monsters: Monsters with high hp/damage that give a massive bonus (gold/XP/stats) when killed by a team.\n",
    "    Dragons: Elite monster which gives team bonus when killed. The 4th dragon killed by a team gives a massive stats bonus. The 5th dragon (Elder Dragon) offers a huge advantage to the team.\n",
    "    Herald: Elite monster which gives stats bonus when killed by the player. It helps to push a lane and destroys structures.\n",
    "    Towers: Structures you have to destroy to reach the enemy Nexus. They give gold.\n",
    "    Level: Champion level. Start at 1. Max is 18.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "data = pd.read_csv('/home/high_diamond_ranked_10min.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # Check data type for all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "    - There are 39 columns and no missing data  \n",
    "    - All columns are already in numeric format  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, data.columns != 'gameId'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['blueWins'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "    - Even distribution of outcome variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract binary columns and check distribution with outcome feature\n",
    "bool_cols = [col for col in data \n",
    "             if np.isin(data[col].dropna().unique(), [0, 1]).all()]\n",
    "binary_data = data[bool_cols]\n",
    "\n",
    "for col in binary_data:\n",
    "    print(pd.crosstab(binary_data['blueWins'], binary_data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "    - All binary features seem to be evenly distrbuted across both outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All continous data\n",
    "cont_data = data.drop(bool_cols, axis = 1 )\n",
    "\n",
    "# Correlation Heatmap \n",
    "plt.figure(figsize=(18,13))\n",
    "sns.heatmap(data.loc[:, data.columns != 'blueWins'].corr(),cbar=True,fmt =' .2f', cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "    - Certain high correlations are expected. Red team deaths is highly correlated with blue team kills and vice versa. Additionally for each team, kills, gold and experience features are highly correlated. This is expected since each kill rewards the team with gold and experience while also allowing the pushing of lanes. Minions, monsters and turrets also provide the team with experince and gold.   \n",
    "    - A lot of these columns are providing the same info in different formats such as blueKills/redDeaths, blueGoldDiff/redGoldDiff, blueExperienceDiff/redExperienceDiff. So these columns can be removed before proceeding to the modeling phase. \n",
    "    - Based on game background, a few other columns could be removed. csPerMin and totalMinionsKilled relate the same information but one is a count and the other is a rate. Same issue with goldPerMin and totalGold. Additionally goldDiff is a ratio between both teams and would be heavily dependent on the other gold features within the dataset. Same with experienceDiff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns \n",
    "extra_col = ['gameId', 'redGoldDiff', 'redExperienceDiff', 'redKills', 'redDeaths', 'blueAvgLevel', 'redAvgLevel', 'redFirstBlood', 'blueGoldPerMin', 'redGoldPerMin', 'redTotalMinionsKilled', 'blueTotalMinionsKilled', 'blueTotalExperience', 'blueTotalGold', 'redTotalGold', 'redTotalExperience', 'redDragons']\n",
    "\n",
    "data_clean = data.drop(extra_col, axis=1)\n",
    "\n",
    "# Redo Correlation Heatmap with excluded data \n",
    "plt.figure(figsize=(18,13))\n",
    "sns.heatmap(data_clean.loc[:, data_clean.columns != 'blueWins'].corr(),cbar=True,fmt =' .2f', cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns for correlation with blueWins\n",
    "corr_list = data_clean.corr()['blueWins'].abs()\n",
    "corr_list = corr_list.sort_values(kind='quicksort')\n",
    "print(corr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "    - Certain features have very little correlation with the outcome so those could be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = ['blueWardsPlaced', 'redWardsPlaced', 'blueWardsDestroyed', 'redWardsDestroyed', 'blueHeralds', 'redHeralds', 'redTowersDestroyed', 'redTotalJungleMinionsKilled', 'blueTowersDestroyed', 'blueTotalJungleMinionsKilled']\n",
    "\n",
    "# Drop features with correlation under 0.20 with the outcome\n",
    "final_data = data_clean.drop(corr_cols, axis=1)\n",
    "final_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data ready for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = final_data.drop('blueWins', axis=1)\n",
    "y = final_data['blueWins']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state=15)\n",
    "\n",
    "accuracy_results = {} # Store all model accuracy ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=400)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "lm_predict_labels = logistic_model.predict(x_test)\n",
    "\n",
    "lm_accuracy = accuracy_score(lm_predict_labels, y_test)\n",
    "print(lm_accuracy)\n",
    "\n",
    "accuracy_results['Logistic Regression'] = lm_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing continous variables\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "scaler.fit(x_train)\n",
    "x_train_normalized = scaler.transform(x_train)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "logistic_model.fit(x_train_normalized, y_train)\n",
    "lm_predict_labels_normalized = logistic_model.predict(x_test_normalized)\n",
    "lm_accuracy_normalized = accuracy_score(lm_predict_labels_normalized, y_test)\n",
    "print(lm_accuracy_normalized)\n",
    "\n",
    "accuracy_results['Logistic Regression Normalized'] = lm_accuracy_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data did not improve the accuracy scores of the model. The next step could be to keep tuning the hyperparameters or try a different model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=1) \n",
    "dt_model.fit(x_train, y_train)\n",
    "dt_predictions = dt_model.predict(x_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(dt_predictions, y_test)\n",
    "print(dt_accuracy)\n",
    "\n",
    "accuracy_results['Decision Tree'] = dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree model also resulted in lower accuracy compared to the original logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)\n",
    "rf_model.fit(x_train, y_train)\n",
    "rf_predictions = rf_model.predict(x_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(rf_predictions, y_test)\n",
    "print(rf_accuracy)\n",
    "\n",
    "accuracy_results['Random Forest'] = rf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Random Forest Parameters\n",
    "rf_model = RandomForestClassifier(n_estimators=75, random_state=1, min_samples_leaf=2)\n",
    "rf_model.fit(x_train, y_train)\n",
    "rf_predictions = rf_model.predict(x_test)\n",
    "\n",
    "rf_accuracy_tuned = accuracy_score(rf_predictions, y_test)\n",
    "print(rf_accuracy_tuned)\n",
    "\n",
    "accuracy_results['Random Forest Tuned'] = rf_accuracy_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the Random Forest model improved accuracy and provided results closer to the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "xgb_predictions = xgb.predict(x_test)\n",
    "xgb_accuracy = accuracy_score(xgb_predictions, y_test)\n",
    "print(xgb_accuracy)\n",
    "\n",
    "accuracy_results['XGBoost'] = xgb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "gnb_predictions = gnb.predict(x_test)\n",
    "gnb_accuracy = accuracy_score(gnb_predictions, y_test)\n",
    "print(gnb_accuracy)\n",
    "\n",
    "accuracy_results['Naive Bayes'] = gnb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results of models\n",
    "print(pd.DataFrame.from_dict(accuracy_results, orient='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, Random Forest and Naive Bayes seem to have the best accurracy scores compared to other models. Moving forward, Logistic Regression will be the model used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficents from the model\n",
    "inferential_table = np.concatenate((logistic_model.coef_, np.exp(logistic_model.coef_)),axis=0)\n",
    "infer_col = final_data.loc[:, final_data.columns != 'blueWins'].columns\n",
    "inferential_data = pd.DataFrame(data=inferential_table, columns=infer_col).T.reset_index().rename(columns={'index': 'Features', 0: 'Coefficient', 1: 'Odds Ratio'})\n",
    "print(inferential_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "    - blueGoldDiff and blueExperienceDiff were the highest predictors of odds of winning the game. blueKills also seem to have high odds of winning the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrix of the predictions \n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, lm_predict_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Model** was chosen as the final predictive model for the dataset. Further tuning of hyperparemeters of other models such as Random Forest or Naive Bayes might have presented similar accuracy scores. blueGoldDiff and blueExperienceDiff seem to be the strongest predictors of game wins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
